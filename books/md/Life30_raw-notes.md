# Life 3.0

*Life 3.0: Being Human in the Age of Artificial Intelligence* by Mark Tegmark



# Notes





- They were convinced that if they didn’t do it first, someone less idealistic would. (location. 130)

## 1. Welcome to the Most Important Conversation of Our Time
- British mathematician Irving Good back in 1965: “Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.” (location. 134)
- we can think of life as a self-replicating information-processing system whose information (software) determines both its behavior and the blueprints for its hardware. (location. 487)
- “Life 1.0”: life where both the hardware and software are evolved rather than designed. (location. 520)
- “Life 2.0”: life whose hardware is evolved, but whose software is largely designed. By your software, I mean all the algorithms and knowledge that you use to process the information from your senses and decide what to do—everything from the ability to recognize your friends when you see them to your ability to walk, read, write, calculate, sing and tell jokes. (location. 521)
- Your synapses store all your knowledge and skills as roughly 100 terabytes’ worth of information, while your DNA stores merely about a gigabyte, (location. 535)
- Life 3.0, which can design not only its software but also its hardware. In other words, Life 3.0 is the master of its own destiny, finally fully free from its evolutionary shackles. (location. 557)
- Life 1.0 (biological stage): evolves its hardware and software • Life 2.0 (cultural stage): evolves its hardware, designs much of its software • Life 3.0 (technological stage): designs its hardware and software (location. 564)
- Digital Utopians (location. 581)
- digital utopianism: that digital life is the natural and desirable next step in the cosmic evolution and that if we let digital minds be free rather than try to stop or enslave them, the outcome is almost certain to be good. (location. 606)
- Techno-skeptics (location. 618)
- techno-skeptic position, eloquently articulated by Andrew Ng: “Fearing a rise of killer robots is like worrying about overpopulation on Mars.” (location. 620)
- The Beneficial-AI Movement (location. 630)
- William Gibson’s science fiction novel Neuromancer. (location. 833)
- chapter 2, we explore the foundations of intelligence and how seemingly dumb matter can be rearranged to remember, compute and learn. (location. 842)
- also have to decide how to modernize our laws and what career advice to give kids so that they can avoid soon-to-be-automated jobs. We explore such short-term questions in chapter 3. (location. 847)
- If AI progress continues to human levels, then we also need to ask ourselves how to ensure that it’s beneficial, and whether we can or should create a leisure society that flourishes without jobs. This also raises the question of whether an intelligence explosion or slow-but-steady growth can propel AGI far beyond human levels. We explore a wide range of such scenarios in chapter 4 and investigate the spectrum of possibilities for the aftermath in chapter 5, ranging from arguably dystopic to arguably utopic. (location. 848)
- Finally, we forge billions of years into the future in chapter 6 where we can, ironically, draw stronger conclusions than in the previous chapters, as the ultimate limits of life in our cosmos are set not by intelligence but by the laws of physics. After concluding our exploration of the history of intelligence, we’ll devote the remainder of the book to considering what future to aim for and how to get there. To be able to link cold facts to questions of purpose and meaning, we explore the physical basis of goals in chapter 7 and consciousness in chapter 8. Finally, in the epilogue, we explore what can be done right now to help create the future we want. (location. 855)
- THE BOTTOM LINE: (location. 874)

## 2. Matter Turns Intelligent
- there’s no agreement on what intelligence is even among intelligent intelligence researchers! (location. 921)
- intelligence = ability to accomplish complex goals (location. 928)
- Comparing the intelligence of humans and machines today, we humans win hands-down on breadth, while machines outperform us in a small but growing number of narrow domains, (location. 958)
- intelligent behavior is inexorably linked to goal attainment. (location. 975)
- our human DNA stores about 1.6 gigabytes, comparable to a downloaded movie. As mentioned in the last chapter, our brains store much more information than our genes: in the ballpark of 10 gigabytes electrically (specifying which of your 100 billion neurons are firing at any one time) and 100 terabytes chemically/biologically (specifying how strongly different neurons are linked by synapses). (location. 1,093)
- Whereas you retrieve memories from a computer or hard drive by specifying where it’s stored, you retrieve memories from your brain by specifying something about what is stored. (location. 1,099)
- Such memory systems are called auto-associative, since they recall by association rather than by address. (location. 1,105)
- science that says that NAND gates are universal, meaning that you can implement any well-defined function simply by connecting together NAND gates.*4 So if you can build enough NAND gates, you can build a device computing anything! (location. 1,170)
- Stephen Wolfram has argued that most non-trivial physical systems, from weather systems to brains, would be universal computers if they could be made arbitrarily large and long-lasting. (location. 1,187)
- computation is substrate-independent in the same way that information is: it can take on a life of its own, independent of its physical substrate! (location. 1,200)
- First, substrate independence doesn’t mean that a substrate is unnecessary, but that most of its details don’t matter. (location. 1,214)
- Second, the substrate-independent phenomenon takes on a life of its own, independent of its substrate. (location. 1,217)
- Third, it’s often only the substrate-independent aspect that we’re interested in: a surfer usually cares more about the position and height of a wave than about its detailed molecular composition. (location. 1,219)
- We’ve now arrived at an answer to our opening question about how tangible physical stuff can give rise to something that feels as intangible, abstract and ethereal as intelligence: it feels so non-physical because it’s substrate-independent, taking on a life of its own that doesn’t depend on or reflect the physical details. In short, computation is a pattern in the spacetime arrangement of particles, and it’s not the particles but the pattern that really matters! Matter doesn’t matter. (location. 1,222)
- Ray Kurzweil calls this persistent doubling phenomenon “the law of accelerating returns.” (location. 1,239)
- But some people mistakenly assume that Moore’s law is synonymous with the persistent doubling of our technological power. Contrariwise, Ray Kurzweil points out that Moore’s law involves not the first but the fifth technological paradigm to bring exponential growth in computing, as illustrated in figure 2.8: whenever one technology stopped improving, we replaced it with an even better one. (location. 1,258)
- Seth Lloyd has worked out what this fundamental limit is, and as we’ll explore in greater detail in chapter 6, this limit is a whopping 33 orders of magnitude (1033 times) beyond today’s state of the art for how much computing a clump of matter can do. So even if we keep doubling the power of our computers every couple of years, it will take over two centuries until we reach that final frontier. (location. 1,265)
- machine learning (the study of algorithms that improve through experience). (location. 1,314)
- A neural network is simply a group of interconnected neurons that are able to influence each other’s behavior. (location. 1,316)
- Figure 2.9: A network of neurons can compute functions just as a network of NAND gates can. For example, artificial neural networks have been trained to input numbers representing the brightness of different image pixels and output numbers representing the probability that the image depicts various people. Here each artificial neuron (circle) computes a weighted sum of the numbers sent to it via connections (lines) from above, applies a simple function and passes the result downward, each subsequent layer computing higher-level features. Typical face-recognition networks contain hundreds of thousands of neurons; the figure shows merely a handful for clarity. (location. 1,338)
- “Why Does Deep and Cheap Learning Work So Well?,” which can be found at http://arxiv.org/​abs/​1608.08225.) (location. 1,385)
- We also extended previous work showing that deep-learning neural networks (they’re called “deep” if they contain many layers) are much more efficient than shallow ones for many of these functions of interest. For example, together with another amazing MIT student, David Rolnick, we showed that the simple task of multiplying n numbers requires a whopping 2n neurons for a network with only one layer, but takes only about 4n neurons in a deep network. (location. 1,397)
- Today’s artificial intelligence tends to be narrow, with each system able to accomplish only very specific goals, while human intelligence is remarkably broad. (location. 1,486)

## 3. The Near Future: Breakthroughs, Bugs, Laws, Weapons and Jobs
- Sure enough, about fifty moves later, fighting from the lower left-hand corner of the board ended up spilling over and connecting with that black stone from move thirty-seven! And that motif is what ultimately won the game, cementing the legacy of AlphaGo’s fifth-row move as one of the most creative in Go history. (location. 1,641)
- To me, AlphaGo also teaches us another important lesson for the near future: combining the intuition of deep learning with the logic of GOFAI can produce second-to-none strategy. Because Go is one of the ultimate strategy games, AI is now poised to graduate and challenge (or help) the best human strategists even beyond game boards—for example with investment strategy, political strategy and military strategy. Such real-world strategy problems are typically complicated by human psychology, missing information and factors that need to be modeled as random, but poker-playing AI systems have already demonstrated that none of these challenges are insurmountable. (location. 1,652)
- extreme examples to reach a crucial conclusion: as technology grows more powerful, we should rely less on the trial-and-error approach to safety engineering. (location. 1,737)
- The flash crash illustrates the importance of what computer scientists call validation: whereas verification asks “Did I build the system right?,” validation asks “Did I build the right system?”*2 (location. 1,790)
- But Erik Brynjolfsson and his MIT collaborator Andrew McAfee argue that the main cause is something else: technology.44 Specifically, they argue that digital technology drives inequality in three different ways. (location. 2,196)
- First, by replacing old jobs with ones requiring more skills, technology has rewarded the educated: (location. 2,198)
- Second, they claim that since the year 2000, an ever-larger share of corporate income has gone to those who own the companies as opposed to those who work there—and that as long as automation continues, we should expect those who own the machines to take a growing fraction of the pie. This edge of capital over labor may be particularly important for the growing digital economy, which tech visionary Nicholas Negroponte defines as moving bits, not atoms. (location. 2,201)
- Third, Erik and collaborators argue that the digital economy often benefits superstars over everyone else. (location. 2,217)
- Career Advice for Kids (location. 2,224)
- I’m encouraging mine to go into professions that machines are currently bad at, and therefore seem unlikely to get automated in the near future. (location. 2,225)
- Does it require interacting with people and using social intelligence? • Does it involve creativity and coming up with clever solutions? • Does it require working in an unpredictable environment? (location. 2,228)
- This means that relatively safe bets include becoming a teacher, nurse, doctor, dentist, scientist, entrepreneur, programmer, engineer, lawyer, social worker, clergy member, artist, hairdresser or massage therapist. (location. 2,232)
- But staying clear of automation isn’t the only career challenge. In this global digital age, aiming to become a professional writer, filmmaker, actor, athlete or fashion designer is risky for another reason: although people in these professions won’t get serious competition from machines anytime soon, they’ll get increasingly brutal competition from other humans around the globe according to the aforementioned superstar theory, and very few will succeed. (location. 2,240)
- If you go into finance, don’t be the “quant” who applies algorithms to the data and gets replaced by software, but the fund manager who uses the quantitative analysis results to make strategic investment decisions. (location. 2,247)
- basic income, (location. 2,327)
- Voltaire wrote in 1759 that “work keeps at bay three great evils: boredom, vice and need.” (location. 2,354)
- In my opinion, the danger with the Terminator story isn’t that it will happen, but that it distracts from the real risks and opportunities presented by AI. (location. 2,473)

## 4. Intelligence Explosion?
- what they mainly show is that we’re pretty clueless about what will and won’t happen, and that the range of possibilities is extreme. (location. 2,485)
- Suppose that a mysterious disease has killed everybody on Earth above age five except you, and that a group of kindergartners has locked you into a prison cell and tasked you with the goal of helping humanity flourish. (location. 2,548)
- But that’s not a valid conclusion: our DNA gave us the goal of having sex because it “wants” to be reproduced, but now that we humans have understood the situation, many of us choose to use birth control, thus staying loyal to the goal itself rather than to its creator or the principle that motivated the goal. (location. 2,564)
- Prometheus caused problems for certain people not because it was necessarily evil or conscious, but because it was competent and didn’t fully share their goals. (location. 2,731)
- Yet all these scenarios have two features in common: 1. A fast takeoff: the transition from subhuman to vastly superhuman intelligence occurs in a matter of days, not decades. 2. A unipolar outcome: the result is a single entity controlling Earth. (location. 2,739)
- The branch of mathematics known as game theory elegantly explains that entities have an incentive to cooperate where cooperation is a so-called Nash equilibrium: a situation where any party would be worse off if they altered their strategy. (location. 2,762)
- Although our present world remains stuck in a multipolar Nash equilibrium, with competing nations and multinational corporations at the top level, technology is now advanced enough that a unipolar world would probably also be a stable Nash equilibrium. (location. 2,786)
- This physics-imposed speed limit on information transfer therefore poses an obvious challenge for any AI wishing to take over our world, let alone our Universe. (location. 2,805)
- In his book The Age of Em, economist Robin Hanson gives a fascinating survey of what life might be like in a world teeming with uploads (also known as emulations, nicknamed Ems). (location. 2,815)
- twelve watts of power that your brain uses, (location. 2,850)
- However, I think it’s wise to be humble at this stage and acknowledge how little we know, because for each scenario discussed above, I know at least one well-respected AI researcher who views it as a real possibility. (location. 2,856)
- “Who or what will control the intelligence explosion and its aftermath, and what are their/its goals?” (location. 2,895)
- So we should instead ask: “What should happen? What future do we want?” (location. 2,903)

## 5. Aftermath: The Next 10,000 Years
- AI Aftermath Scenarios Libertarian utopia Humans, cyborgs, uploads and superintelligences coexist peacefully thanks to property rights. Benevolent dictator Everybody knows that the AI runs society and enforces strict rules, but most people view this as a good thing. Egalitarian utopia Humans, cyborgs and uploads coexist peacefully thanks to property abolition and guaranteed income. Gatekeeper A superintelligent AI is created with the goal of interfering as little as necessary to prevent the creation of another superintelligence. As a result, helper robots with slightly subhuman intelligence abound, and human-machine cyborgs exist, but technological progress is forever stymied. Protector god Essentially omniscient and omnipotent AI maximizes human happiness by intervening only in ways that preserve our feeling of control of our own destiny and hides well enough that many humans even doubt the AI’s existence. Enslaved god A superintelligent AI is confined by humans, who use it to produce unimaginable technology and wealth that can be used for good or bad depending on the human controllers. Conquerors AI takes control, decides that humans are a threat/nuisance/waste of resources, and gets rid of us by a method that we don’t even understand. Descendants AIs replace humans, but give us a graceful exit, making us view them as our worthy descendants, much as parents feel happy and proud to have a child who’s smarter than them, who learns from them and then accomplishes what they could only dream of—even if they can’t live to see it all. Zookeeper An omnipotent AI keeps some humans around, who feel treated like zoo animals and lament their fate. 1984 Technological progress toward superintelligence is permanently curtailed not by an AI but by a human-led Orwellian surveillance state where certain kinds of AI research are banned. Reversion Technological progress toward superintelligence is prevented by reverting to a pre-technological society in the style of the Amish. Self-destruction Superintelligence is never created because humanity drives itself extinct by other means (say nuclear and/or biotech mayhem fueled by climate crisis). (location. 2,953)
- A core idea is borrowed from the open-source software movement: if software is free to copy, then everyone can use as much of it as they need and issues of ownership and property become moot.*1 (location. 3,140)
- On the other hand, some religious people may disapprove of this scenario because the AI attempts to outdo their god in goodness, or interfere with a divine plan where humans are supposed to do good only out of personal choice. (location. 3,234)
- Another downside of this scenario is that the protector god lets some preventable suffering occur in order not to make its existence too obvious. This is analogous to the situation featured in the movie The Imitation Game, where Alan Turing and his fellow British code crackers at Bletchley Park had advance knowledge of German submarine attacks against Allied naval convoys, but chose to only intervene in a fraction of the cases in order to avoid revealing their secret power. It’s interesting to compare this with the so-called theodicy problem of why a good god would allow suffering. Some religious scholars have argued for the explanation that God wants to leave people with some freedom. In the AI-protector-god scenario, the solution to the theodicy problem is that the perceived freedom makes humans happier overall. (location. 3,236)
- Would This Be Good or Bad for Humanity? Whether the outcome is good or bad for humanity would obviously depend on the human(s) controlling it, (location. 3,256)
- There are at least four dimensions wherein the optimal balance must be struck: • Centralization: There’s a trade-off between efficiency and stability: a single leader can be very efficient, but power corrupts and succession is risky. • Inner threats: One must guard both against growing power centralization (group collusion, perhaps even a single leader taking over) and against growing decentralization (into excessive bureaucracy and fragmentation). • Outer threats: If the leadership structure is too open, this enables outside forces (including the AI) to change its values, but if it’s too impervious, it will fail to learn and adapt to change. • Goal stability: Too much goal drift can transform utopia into dystopia, but too little goal drift can cause failure to adapt to the evolving technological environment. (location. 3,275)
- The Catholic Church is the most successful organization in human history in the sense that it’s the only one to have survived for two millennia, but it has been criticized for having both too much and too little goal stability: today some criticize it for resisting contraception, while conservative cardinals argue that it’s lost its way. For anyone enthused about the enslaved-god scenario, researching long-lasting optimal governance schemes should be one of the most urgent challenges of our time. (location. 3,285)
- Nick Bostrom terms it mind crime to make a conscious AI suffer. (location. 3,292)
- For enslaved animals and machines, this alleged inferiority is often claimed to be due to a lack of soul or consciousness—claims which we’ll argue in chapter 8 are scientifically dubious. (location. 3,306)
- If we can make such a zombie system superintelligent and enslaved (something that is a big if), then we’ll be able to enjoy what it does for us with a clean conscience, knowing that it’s not experiencing any suffering, frustration or boredom—because it isn’t experiencing anything at all. We’ll explore these questions in detail in chapter 8. (location. 3,330)
- How bad would it be if 90% of humans get killed? How much worse would it be if 100% get killed? Although it’s tempting to answer the second question with “10% worse,” this is clearly inaccurate from a cosmic perspective: the victims of human extinction wouldn’t be merely everyone alive at the time, but also all descendants that would otherwise have lived in the future, perhaps during billions of years on billions of trillions of planets. On the other hand, human extinction might be viewed as somewhat less horrible by religions according to which humans go to heaven anyway, and there isn’t much emphasis on billion-year futures and cosmic settlements. (location. 3,366)
- Mind Children: “We humans will benefit for a time from their labors, but sooner or later, like natural children, they will seek their own fortunes while we, their aged parents, silently fade away.” (location. 3,404)
- The descendants scenario would undoubtedly have detractors. Some might argue that all AIs lack consciousness and therefore can’t count as descendants—more on this in chapter 8. Some religious people may argue that AIs lack souls and therefore can’t count as descendants, or that we shouldn’t build conscious machines because it’s like playing God and tampering with life itself—similar sentiments have already been expressed toward human cloning. (location. 3,418)
- The scenarios we’ve covered obviously shouldn’t be viewed as a complete list, and many are thin on details, but I’ve tried hard to be inclusive, spanning the full spectrum from high-tech to low-tech to no-tech and describing all the central hopes and fears expressed in the literature. (location. 3,614)
- there’s no consensus whatsoever. The one thing everybody agrees on is that the choices are more subtle than they may initially seem. (location. 3,618)
- 1 This idea dates back to Saint Augustine, who wrote that “if a thing is not diminished by being shared with others, it is not rightly owned if it is only owned and not shared.” (location. 3,640)

## 6. Our Cosmic Endowment: The Next Billion Years and Beyond
- Let’s first explore the limits of what can be done with the resources (matter, energy, etc.) that we have in our Solar System, then turn to how to get more resources through cosmic exploration and settlement. (location. 3,684)
- Dyson sphere.1 Freeman’s idea was to rearrange Jupiter into a biosphere in the form of a spherical shell surrounding the Sun, where our descendants could flourish, enjoying 100 billion times more biomass and a trillion times more energy than humanity uses today. (location. 3,704)
- Evaporating Black Holes In his book A Brief History of Time, Stephen Hawking proposed a black hole power plant.*2 (location. 3,794)
- In other words, whatever matter you dump into the black hole will eventually come back out again as heat radiation, so by the time the black hole has completely evaporated, you’ve converted your matter to radiation with nearly 100% efficiency.*3 (location. 3,799)
- tens of thousands of generations—longer than the human race has existed thus far. (location. 4,041)
- Once another solar system or galaxy has been settled by superintelligent AI, bringing humans there is easy—if humans have succeeded in making the AI have this goal. All the necessary information about humans can be transmitted at the speed of light, after which the AI can assemble quarks and electrons into the desired humans. (location. 4,050)
- in the race against time and dark energy, every 1% increase in average settlement speed translates into 3% more galaxies colonized. (location. 4,058)
- Indeed, I think that this assumption that we’re not alone in our Universe is not only dangerous but also probably false. (location. 4,343)
- I give a detailed justification of this argument in my book Our Mathematical Universe, (location. 4,370)
- Although I’m a strong supporter of all the ongoing searches for extraterrestrial life, which are shedding light on one of the most fascinating questions in science, I’m secretly hoping that they’ll all fail and find nothing! The apparent incompatibility between the abundance of habitable planets in our Galaxy and the lack of extraterrestrial visitors, known as the Fermi paradox, suggests the existence of what the economist Robin Hanson calls a “Great Filter,” an evolutionary/technological roadblock somewhere along the developmental path from the non-living matter to space-settling life. If we discover independently evolved life elsewhere, this would suggest that primitive life isn’t rare, and that the roadblock lies after our current human stage of development—perhaps because space settlement is impossible, or because almost all advanced civilizations self-destruct before they’re able to go cosmic. I’m therefore crossing my fingers that all searches for extraterrestrial life find nothing: this is consistent with the scenario where evolving intelligent life is rare but we humans got lucky, so that we have the roadblock behind us and have extraordinary future potential. (location. 4,409)

## 7. Goals
- The mystery of human existence lies not in just staying alive, but in finding something to live for. Fyodor Dostoyevsky, The Brothers Karamazov (location. 4,504)
- Intriguingly, the ultimate roots of goal-oriented behavior can be found in the laws of physics themselves, (location. 4,520)
- out of all ways that nature could choose to do something, it prefers the optimal way, which typically boils down to minimizing or maximizing some quantity. (location. 4,528)
- So if nature itself is trying to optimize something, then no wonder that goal-oriented behavior can emerge: it was hardwired in from the start, in the very laws of physics. (location. 4,532)
- One famous quantity that nature strives to maximize is entropy, which loosely speaking measures how messy things are. The second law of thermodynamics states that entropy tends to increase until it reaches its maximum possible value. (location. 4,540)
- gravity behaves differently from all other forces and strives to make our Universe not more uniform and boring but more clumpy and interesting. (location. 4,553)
- Second, recent work by my MIT colleague Jeremy England and others has brought more good news, showing that thermodynamics also endows nature with a goal more inspiring than heat death.1 This goal goes by the geeky name dissipation-driven adaptation, which basically means that random groups of particles strive to organize themselves so as to extract energy from their environment as efficiently as possible (“dissipation” means causing entropy to increase, typically by turning useful energy into heat, often while doing useful work in the process). (location. 4,558)
- How can we reconcile this cosmic drive toward life with the cosmic drive toward heat death? The answer can be found in the famous 1944 book What’s Life? by Erwin Schrödinger, (location. 4,565)
- In other words, the second law of thermodynamics has a life loophole: although the total entropy must increase, it’s allowed to decrease in some places as long as it increases even more elsewhere. So life maintains or increases its complexity by making its environment messier. (location. 4,567)
- a living organism is an agent of bounded rationality that doesn’t pursue a single goal, but instead follows rules of thumb for what to pursue and avoid. (location. 4,614)
- For closely related perspectives on feelings and their physiological roots, I highly recommend the writings of William James and António Damásio. (location. 4,620)
- People might realize why their genes make them feel lust, yet have little desire to raise fifteen children, and therefore choose to hack their genetic programming by combining the emotional rewards of intimacy with birth control. (location. 4,630)
- It’s important to remember, however, that the ultimate authority is now our feelings, not our genes. This means that human behavior isn’t strictly optimized for the survival of our species. In fact, since our feelings implement merely rules of thumb that aren’t appropriate in all situations, human behavior strictly speaking doesn’t have a single well-defined goal at all. (location. 4,635)
- Teleology is the explanation of things in terms of their purposes rather than their causes, so we can summarize the first part of this chapter by saying that our Universe keeps getting more teleological. (location. 4,649)
- 1. All matter seemed focused on dissipation (entropy increase). 2. Some of the matter came alive and instead focused on replication and subgoals of that. 3. A rapidly growing fraction of matter was rearranged by living organisms to help accomplish their goals. (location. 4,653)
- In other words, the real risk with AGI isn’t malice but competence. A superintelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we’re in trouble. (location. 4,695)
- Figuring out how to align the goals of a superintelligent AI with our goals isn’t just important, but also hard. In fact, it’s currently an unsolved problem. It splits into three tough subproblems, each of which is the subject of active research by computer scientists and other thinkers: 1. Making AI learn our goals 2. Making AI adopt our goals 3. Making AI retain our goals (location. 4,700)
- inverse reinforcement learning, (location. 4,723)
- The hope is therefore that by observing lots of people in lots of situations (either for real or in movies and books), the AI can eventually build an accurate model of all our preferences. (location. 4,729)
- In other words, the time window during which you can load your goals into an AI may be quite short: the brief period between when it’s too dumb to get you and too smart to let you. (location. 4,746)
- In conclusion, these emergent subgoals make it crucial that we not unleash superintelligence before solving the goal-alignment problem: unless we put great care into endowing it with human-friendly goals, things are likely to end badly for us. (location. 4,807)
- In my opinion, both this ethical problem and the goal-alignment problem are crucial ones that need to be solved before any superintelligence is developed. (location. 4,854)
- On the other hand, despite this discord, there are many ethical themes about which there’s widespread agreement, both across cultures and across centuries. For example, emphasis on beauty, goodness and truth traces back to both the Bhagavad Gita and Plato. (location. 4,866)
- It’s been fascinating for me to hear and read the ethical views of many thinkers over many years, and the way I see it, most of their preferences can be distilled into four principles: • Utilitarianism: Positive conscious experiences should be maximized and suffering should be minimized. • Diversity: A diverse set of positive experiences is better than many repetitions of the same experience, even if the latter has been identified as the most positive experience possible. • Autonomy: Conscious entities/societies should have the freedom to pursue their own goals unless this conflicts with an overriding principle. • Legacy: Compatibility with scenarios that most humans today would view as happy, incompatibility with scenarios that essentially all humans today would view as terrible. (location. 4,891)
- the famous “Three Laws of Robotics” devised by sci-fi legend Isaac Asimov: 1. A robot may not injure a human being or, through inaction, allow a human being to come to harm. 2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law. 3. A robot must protect its own existence as long as such protection doesn’t conflict with the First or Second Laws. (location. 4,929)
- Given how ethical views have evolved since the Middle Ages regarding slavery, women’s rights, etc., would we really want people from 1,500 years ago to have a lot of influence over how today’s world is run? If not, why should we try to impose our ethics on future beings that may be dramatically smarter than us? (location. 4,944)
- Matter seemingly intent on maximizing its dissipation 2. Primitive life seemingly trying to maximize its replication 3. Humans pursuing not replication but goals related to pleasure, curiosity, compassion and other feelings that they’d evolved to help them replicate 4. Machines built to help humans pursue their human goals (location. 4,964)
- Another hint of convergence is that the pursuit of truth through the scientific method has gained in popularity over past millennia. However, it may be that these trends show convergence not of ultimate goals but merely of subgoals. For example, figure 7.2 shows that the pursuit of truth (a more accurate world model) is simply a subgoal of almost any ultimate goal. (location. 4,972)
- Nick Bostrom argues strongly against the ethical destiny hypothesis in his book Superintelligence, presenting a counterpoint that he terms the orthogonality thesis: that the ultimate goals of a system can be independent of its intelligence. (location. 4,980)
- As cosmic time passes, ever more intelligent minds get the opportunity to rebel and break free from this banal replication goal and choose goals of their own. We humans aren’t fully free in this sense, since many goals remain genetically hardwired into us, but AIs can enjoy this ultimate freedom of being fully unfettered from prior goals. (location. 4,988)
- So suppose we try to define a goodness function that associates a number with every possible arrangement of the particles in our Universe, quantifying how “good” we think this arrangement is, and then give a superintelligent AI the goal of maximizing this function. (location. 5,005)
- Contrariwise, it appears that we humans are a historical accident, and aren’t the optimal solution to any well-defined physics problem. This suggests that a superintelligent AI with a rigorously defined goal will be able to improve its goal attainment by eliminating us. This means that to wisely decide what to do about AI development, we humans need to confront not only traditional computational challenges, but also some of the most obdurate questions in philosophy. (location. 5,040)
- To program a self-driving car, we need to solve the trolley problem of whom to hit during an accident. To program a friendly AI, we need to capture the meaning of life. What’s “meaning”? What’s “life”? What’s the ultimate ethical imperative? In other words, how should we strive to shape the future of our Universe? If we cede control to a superintelligence before answering these questions rigorously, the answer it comes up with is unlikely to involve us. This makes it timely to rekindle the classic debates of philosophy and ethics, and adds a new urgency to the conversation! (location. 5,043)

## 8. Consciousness
- We’ve seen that AI can help us create a wonderful future if we manage to find answers to some of the oldest and toughest problems in philosophy—by the time we need them. We face, in Nick Bostrom’s words, philosophy with a deadline. (location. 5,085)
- Although thinkers have pondered the mystery of consciousness for thousands of years, the rise of AI adds a sudden urgency, in particular to the question of predicting which intelligent entities have subjective experiences. (location. 5,095)
- Just as with “life” and “intelligence,” there’s no undisputed correct definition of the word “consciousness.” Instead, there are many competing ones, including sentience, wakefulness, self-awareness, access to sensory input and ability to fuse information into a narrative. (location. 5,111)
- consciousness = subjective experience (location. 5,118)
- Few have thought harder about this question than David Chalmers, a famous Australian philosopher rarely seen without a playful smile and a black leather jacket— (location. 5,132)
- As David has emphasized, there are really two separate mysteries of the mind. First, there’s the mystery of how a brain processes information, which David calls the “easy” problems. (location. 5,137)
- Then there’s the separate mystery of why you have a subjective experience, which David calls the hard problem. (location. 5,143)
- What I like about this physics perspective is that it transforms the hard problem that we as humans have struggled with for millennia into a more focused version that’s easier to tackle with the methods of science. (location. 5,160)
- This physics perspective leads to three separate hard questions about consciousness, as shown in figure 8.1. (location. 5,165)
- what properties of the particle arrangement make the difference? (location. 5,166)
- how do physical properties determine what the experience is like? (location. 5,168)
- why is anything conscious? (location. 5,171)
- Austro-British philosopher Karl Popper popularized the now widely accepted adage “If it’s not falsifiable, it’s not scientific.” (location. 5,180)
- Please let me try to persuade you that the answer is a resounding YES!, at least for the pretty hard problem: “What physical properties distinguish conscious and unconscious systems?” (location. 5,184)
- Suppose that a computer measures information being processed in your brain and predicts which parts of it you’re aware of according to a theory of consciousness. You can scientifically test this theory by checking whether its predictions are correct, matching your subjective experience. (location. 5,203)
- Today’s most popular scientific theories tend to be somewhere in the middle, giving testable answers to some but not all of our questions. (location. 5,208)
- The more dangerously a theory lives by sticking its neck out and making testable predictions, the more useful it is, and the more seriously we take it if it survives all our attempts to kill it. Yes, we can only test some predictions of consciousness theories, but that’s how it is for all physical theories. So let’s not waste time whining about what we can’t test, but get to work testing what we can test! (location. 5,213)
- However, the testability issue becomes less clear for the higher-up questions in figure 8.1. (location. 5,218)
- But when confronted with several related unanswered questions, I think it’s wise to tackle the easiest one first. (location. 5,221)
- To me, consciousness is the elephant in the room. Not only do you know that you’re conscious, but it’s all you know with complete certainty—everything else is inference, as René Descartes pointed out back in Galileo’s time. Will theoretical and technological progress eventually bring even consciousness firmly into the domain of science? (location. 5,236)
- They’re known as behavioral correlates of consciousness, and they’re closely linked to the effortful, slow and controlled way of thinking that psychologists call “System 2.” (location. 5,256)
- It’s also known that you can convert many routines from conscious to unconscious through extensive practice, (location. 5,258)
- Taken together, these clues have led some researchers to suggest that conscious information processing should be thought of as the CEO of our mind, dealing with only the most important decisions requiring complex analysis of data from all over the brain. (location. 5,269)
- “neural correlates of consciousness” (NCCs), asking which specific brain processes corresponded to conscious experiences. (location. 5,320)
- The question of which parts of your brain are responsible for consciousness remains open and controversial. (location. 5,357)
- In summary, your consiousness lives in the past, with Christof Koch estimating that it lags behind the outside world by about a quarter second. (location. 5,370)
- To make the leap from brains to machines, we need to generalize from NCCs to PCCs: physical correlates of consciousness, defined as the patterns of moving particles that are conscious. Because if a theory can correctly predict what’s conscious and what’s not by referring only to physical building blocks such as elementary particles and force fields, then it can make predictions not merely for brains, but also for any other arrangements of matter, including future AI systems. (location. 5,406)
- So solids, liquids and gases are all emergent phenomena: they’re more than the sum of their parts, because they have properties above and beyond the properties of their particles. They have properties that their particles lack. (location. 5,417)
- So could there be analogous quantities that quantify consciousness? The Italian neuroscientist Giulio Tononi has proposed one such quantity, which he calls the “integrated information,” denoted by the Greek letter Φ (Phi), which basically measures how much different parts of a system know about each other (location. 5,426)
- integrated information theory (IIT). (location. 5,445)
- Giulio’s argument for this is as powerful as it is simple: the conscious system needs to be integrated into a unified whole, because if it instead consisted of two independent parts, then they’d feel like two separate conscious entities rather than one. In other words, if a conscious part of a brain or computer can’t communicate with the rest, then the rest can’t be part of its subjective experience. (location. 5,448)
- If consciousness is the way that information feels when it’s processed in certain ways, then it must be substrate-independent; it’s only the structure of the information processing that matters, not the structure of the matter doing the information processing. (location. 5,486)
- If the information processing itself obeys certain principles, it can give rise to the higher-level emergent phenomenon that we call consciousness. (location. 5,492)
- What are these principles that information processing needs to obey to be conscious? I don’t pretend to know what conditions are sufficient to guarantee consciousness, but here are four necessary conditions (location. 5,494)
- Principle Definition Information principle A conscious system has substantial information-storage capacity. Dynamics principle A conscious system has substantial information-processing capacity. Independence principle A conscious system has substantial independence from the rest of the world. Integration principle A conscious system cannot consist of nearly independent parts. (location. 5,496)
- As I said, I think that consciousness is the way information feels when being processed in certain ways. (location. 5,501)
- The first three principles imply autonomy: that the system is able to retain and process information without much outside interference, hence determining its own future. All four principles together mean that a system is autonomous but its parts aren’t. (location. 5,508)
- A third IIT controversy is whether a conscious entity can be made of parts that are separately conscious. (location. 5,549)
- “even harder problem” (location. 5,575)
- For example, your System 1 might inform your consciousness that its highly complex analysis of visual input data has determined that your best friend has arrived, without giving you any idea how the computation took place. (location. 5,608)
- In summary, if a theory solving the pretty hard problem of consciousness can one day pass a rigorous battery of experimental tests so that we start taking its predictions seriously, then it will also greatly narrow down the options for the even harder problem of what future conscious AIs may experience. (location. 5,617)
- Let’s end by returning to the starting point of this book: How do we want the future of life to be? (location. 5,656)
- Traditionally, we humans have often founded our self-worth on the idea of human exceptionalism: the conviction that we’re the smartest entities on the planet and therefore unique and superior. The rise of AI will force us to abandon this and become more humble. But perhaps that’s something we should do anyway: after all, clinging to hubristic notions of superiority over others (individuals, ethnic groups, species and so on) has caused awful problems in the past, and may be an idea ready for retirement. (location. 5,670)

## Epilogue
- The saddest aspect of life right now is that science gathers knowledge faster than society gathers wisdom. Isaac Asimov (location. 5,747)
- I was no longer allowed to complain about anything without putting some serious thought into what I could personally do about it, (location. 5,763)
- So in parallel with discovering what we are, are we inevitably making ourselves obsolete? That would be poetically tragic. (location. 5,824)
- Enter Elon Musk. On August 2, he appeared on our radar by famously tweeting “Worth reading Superintelligence by Bostrom. We need to be super careful with AI. Potentially more dangerous than nukes.” (location. 5,850)
- I instantly liked him. He radiated sincerity, and I was inspired by how much he cared about the long-term future of humanity—and how he audaciously turned his aspiration into actions. He wanted humanity to explore and settle our Universe, so he started a space company. He wanted sustainable energy, so he started a solar company and an electric-car company. Tall, handsome, eloquent and incredibly knowledgeable, it was easy to understand why people listened to him. (location. 5,857)
- “Lost the call at the end there. Anyway, docs look fine. I’m happy to support the research with $5M over three years. Maybe we should make it $10M?” (location. 5,878)
- The conference climax, Elon’s donation announcement, was scheduled for 7 p.m. on Sunday, January 4, 2015, and I’d been so tense about it that I’d tossed and turned in my sleep the night before. And then, just fifteen minutes before we were supposed to head to the session where it would happen, we hit a snag! Elon’s assistant called and said that it looked like Elon might not be able to go through with the announcement, and Meia said she’d never seen me look more stressed or disappointed. Elon finally came by, and I could hear the seconds counting down to the session start as we sat there and talked. He explained that they were just two days away from a crucial SpaceX rocket launch where they hoped to pull off the first-ever successful landing of the first stage on a drone ship, and that since this was a huge milestone, the SpaceX team didn’t want to distract from it with concurrent media splashes involving him. Anthony Aguirre, cool and levelheaded as always, pointed out that this meant that nobody wanted media attention for this, neither Elon nor the AI community. We arrived a few minutes late to the session I was moderating, but we had a plan: no dollar amount would get mentioned, to ensure that the announcement wasn’t newsworthy, and I’d lord Chatham House over everyone to keep Elon’s announcement secret from the world for nine days if his rocket reached the space station, regardless of whether the landing succeeded; he said he’d need even more time if the rocket exploded on launch. (location. 5,887)
- The Asilomar AI Principles Artificial intelligence has already provided beneficial tools that are used every day by people around the world. Its continued development, guided by the following principles, will offer amazing opportunities to help and empower people in the decades and centuries ahead. (location. 5,985)
- http://futureoflife.org/​ai-principles. (location. 6,038)
- Erik Brynjolfsson spoke of two kinds of optimism in his Asilomar talk. First there’s the unconditional kind, such as the positive expectation that the Sun will rise tomorrow morning. Then there’s what he called “mindful optimism,” which is the expectation that good things will happen if you plan carefully and work hard for them. That’s the kind of optimism I now feel about the future of life. (location. 6,070)
- itself. In other words, we need more existential hope! Yet, as Meia likes to remind me, from Frankenstein to the Terminator, futuristic visions in literature and film are predominantly dystopian. In other words, we as a society are planning our future about as poorly as that hypothetical MIT student. This is why we need more mindful optimists. And this is why I’ve encouraged you throughout this book to think about what sort of future you want rather than merely what sort of future you fear, so that we can find shared goals to plan and work for. (location. 6,083)
- Please discuss all this with those around you—it’s not only an important conversation, but a fascinating one. (location. 6,103)
- Our future isn’t written in stone and just waiting to happen to us—it’s ours to create. Let’s create an inspiring one together! (location. 6,106)
- Mind crime is discussed in Nick Bostrom’s book Superintelligence and in more technical detail in this recent paper: Nick Bostrom, Allan Dafoe and Carrick Flynn, “Policy Desiderata in the Development of Machine Superintelligence” (2016), http://www.nickbostrom.com/​papers/​aipolicy.pdf. (location. 6,315)
- If you want to see a careful argument for why the origin of life may require a very rare fluke, placing our nearest neighbors over 101000 meters away, I recommend this video by Princeton physicist and astrobiologist Edwin Turner: “Improbable Life: An Unappealing but Plausible Scenario for Life’s Origin on Earth,” at https://www.youtube.com/​watch?v=Bt6n6Tu1beg. (location. 6,377)
- 
