# Life 3.0

*Life 3.0: Being Human in the Age of Artificial Intelligence* by Mark Tegmark



# Notes

## General Outline (45)
1. **Welcome to the Most Important Conversation of Our Time**: introduction, terms, what is at stake
2. **Matter Turns Intelligent**: explore the foundations of intelligence and how seemingly dumb matter can be rearranged to remember, compute and learn
3. **The Near Future: Breakthroughs, Bugs, Laws, Weapons and Jobs**: how to modernize our laws and what career advice to give kids so that they can avoid soon-to-be-automated jobs
4. **Intelligence Explosion?**: how to ensure that AGI is beneficial, whether we can or should create a leisure society that flourishes without jobs, and if an intelligence explosion can propel AGI far beyond human levels. 
5. **Aftermath: The Next 10,000 Years**: explore different scenarios of how AGI could unfold.
6. **Our Cosmic Endowment: The Next Billion Years and Beyond**: examine the laws of physics that will determine how the universe unfolds
7. **Goals**: explore the physical basis of goals
8. **Consciousness**: explore the physical basis of consciousness
9. **Epilogue**: what we can do now

## Terminology (39)
| Term | Definition |
| -------- | -------- |
| **Life** | Process that can retain its complexity and replicate |
| **Life 1.0** | Life that evolves its hardware and software (biological stage) |
| **Life 2.0** | Life that evolves its hardware bust designs much of its software (cultural stage) |
| **Life 3.0** | Life that designs its hardware and software (technological stage) |
| **Intelligence** | Ability to accomplish complex goals |
| **Artificial Intelligence (AI)** | Non-biological intelligence |
| **Narrow intelligence** | Ability to accomplish a narrow set of goals (play chess, drive car) |
| **General intelligence** | Ability to accomplish virtually any goal, including learning |
| **Universal intelligence** | Ability to acquire general intelligence given access to data and resources |
| **Artificial General Intelligence (AGI)** | Ability to accomplish any cognitive task at least as well as humans |
| **Human-level AI** | AGI |
| **Strong AI** | AGI |
| **Superintelligence** | General intelligence far beyond human level |
| **Civilization** | Interacting group of intelligent life forms |
| **Consciousness** | Subjective experience |
| **Qualia** | Individual instances of subjective experience |
| **Ethics** | Principles that govern how we should behave |
| **Teleology** | Explanation of things in terms of their goals or purposes rather than their causes |
| **Goal-oriented behavior** | Behavior more easily explained via its effect than via its cause |
| **Having a goal** | Exhibiting goal-oriented behavior |
| **Having purpose** | Serving goals of one's own or of another entity |
| **Friendly AI** | Superintelligence whose goals are aligned with ours |
| **Cyborg** | Human-machine hybrid  |
| **Intelligence explosion** | Recursive self-improvment rapidly leading to superintelligence |
| **Singularity** | Intelligence explosion |
| **Universe** | The region of space from which light has had time to reach us during the 13.8 buillion years since our Big Bang |

## Prelude: Team Omega
- He opens with a fictional tale about an alturistic group that greated a super intelligent AI as an account of what could happen: "They were convinced that if they didn’t do it first, someone less idealistic would." (location. 130)
- British mathematician Irving Good back in 1965: “Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.” (location. 134)

## 1. Welcome to the Most Important Conversation of Our 

### A Brief History of Complexity
- (a brief history of the origins of the universe)

### The Three Stages of Life
- **Life**: "a self-replicating information-processing system whose information (software) determines both its behavior and the blueprints for its hardware." (25)
	- **Life 1.0** (biological stage): life where both the hardware and software are evolved rather than designed
	- **Life 2.0** (cultural stage): life whose hardware is evolved, but whose software is largely designed. By your software, I mean all the algorithms and knowledge that you use to process the information from your senses and decide what to do—everything from the ability to recognize your friends when you see them to your ability to walk, read, write, calculate, sing and tell jokes. 
	- **Life 3.0** (technological stage): life which can design not only its software but also its hardware. "Life 3.0 is the master of its own destiny, finally fully free from its evolutionary shackles." (29)

### Controversies
- **Artificial General Intelligence**: artifical intelligence which can accomplish virtually any goal, including learning—this serves as his working definition when referring to AI throughout the book (30)
- Most people fall into one of the categories shown in [Figure 1.2 (31)](https://books.google.com/books?id=70EzDwAAQBAJ&pg=PA30&lpg=PA30&dq=techno-skeptics+life+3.0&source=bl&ots=jLTwmA0TGA&sig=GtnPmdO_a7xZqzeDByGH7xUx0fQ&hl=en&sa=X&ved=0ahUKEwiS4-2Mjd_WAhUC5IMKHXIgBncQ6AEINDAC#v=onepage&q&f=false):
	- **Digital utopianism**: "that digital life is the natural and desirable next step in the cosmic evolution and that if we let digital minds be free rather than try to stop or enslave them, the outcome is almost certain to be good." (32)
	- **Techno-skeptic position**: articulated by Andrew Ng: “Fearing a rise of killer robots is like worrying about overpopulation on Mars.” (32-33)
	- **The Beneficial-AI Movement**: the position he and his Future of Life Institute take

### Misconceptions
- It is often difficult to have a converstaion about AI without talking past one antoher due to misunderstanding. See the table above for Tegmark's definitions of terms he uses. Other misconceptions he discusses include:
	- **Timeline**: "superintelligence may happen in decades, centuries, or never: AI experts disagree and we simply don't know" (41)
	- **Controversy**: "many top AI researchers are worried about AI" (42)
	- **Risks**: "human-killing robots" are less a concern than (1) competence: the machine's ability to carry out the human's goals, and (2) goals: aligning AI goals with human goals and choosing between competing human goals


## 2. Matter Turns

### What Is Intelligence?
- there’s no agreement on what intelligence is even among intelligent intelligence researchers! (49)
- **Intelligence**: the ability to accomplish complex goals (50)
- Comparing the intelligence of humans and machines today, we humans win hands-down on breadth, while machines outperform us in a small but growing number of narrow domains (52)
- intelligent behavior is inexorably linked to goal attainment. (53)

### What Is Memory?
- our human DNA stores about 1.6 gigabytes, comparable to a downloaded movie. As mentioned in the last chapter, our brains store much more information than our genes: in the ballpark of 10 gigabytes electrically (specifying which of your 100 billion neurons are firing at any one time) and 100 terabytes chemically/biologically (specifying how strongly different neurons are linked by synapses). (60)
- **Auto-associative memory**: retrieve memories from your brain by specifying something about what is stored, as compared with how you retrieve memories from a computer or hard drive by specifying where it’s stored (60)

### What Is Computation?
- NAND gates are universal: you can implement any well-defined function simply by connecting together NAND gates. So if you can build enough NAND gates, you can build a device computing anything (64)
- Stephen Wolfram has argued that most non-trivial physical systems, from weather systems to brains, would be universal computers if they could be made arbitrarily large and long-lasting. (65)
- computation is substrate-independent in the same way that information is: it can take on a life of its own, independent of its physical substrate (65-66)
	- a substrate is necessary, but most of its details don’t matter
	- the substrate-independent phenomenon takes on a life of its own, independent of its substrate
	- it’s often only the substrate-independent aspect that we’re interested in: a surfer usually cares more about the position and height of a wave than about its detailed molecular composition
	- computation is a pattern in the spacetime arrangement of particles, and it’s not the particles but the pattern that really matters: matter doesn’t matter (67)
	- substrate independence allows you to keep the same software as hardware improves: Seth Lloyd has worked out what this fundamental limit is, and as we’ll explore in greater detail in chapter 6, this limit is a whopping 33 orders of magnitude (1033 times) beyond today’s state of the art for how much computing a clump of matter can do. So even if we keep doubling the power of our computers every couple of years, it will take over two centuries until we reach that final frontier. (69)

### What Is Learning?
- **Machine learning**: the study of algorithms that improve through experience (71)
- **Neural network**: a group of interconnected neurons that are able to influence each other’s behavior. (71)
	- A network of neurons can compute functions just as a network of NAND gates can. For example, artificial neural networks have been trained to input numbers representing the brightness of different image pixels and output numbers representing the probability that the image depicts various people.
	- **Deep neural network**: Deep neural networks (neural networks with many layers) are much more efficient than shallow ones for many of these functions of interest. For example, together with another amazing MIT student, David Rolnick, we showed that the simple task of multiplying n numbers requires a whopping 2n neurons for a network with only one layer, but takes only about 4n neurons in a deep network. (76)

## 3. The Near Future: Breakthroughs, Bugs, Laws, Weapons and Jobs

### Breakthroughs
- AI has exhibited the ability to find "creative" solutions to problems that humans have not:
	- Maximize Atari computer game scores by breaking through on the side to the ball bounces around the top (83)
	- Playing a Go game on the fifth line (humans keep to the third or forth). The move was called "one of the most creative in Go history" (88)
	- **Strategy**: To me, AlphaGo also teaches us another important lesson for the near future: combining the intuition of deep learning with the logic of GOFAI can produce second-to-none **strategy**. Because Go is one of the ultimate strategy games, AI is now poised to graduate and challenge (or help) the best human strategists even beyond game boards—for example with investment strategy, political strategy and military strategy. Such real-world strategy problems are typically complicated by human psychology, missing information and factors that need to be modeled as random, but...none of these challenges are insurmountable. (89)

### Bugs vs. Robust AI
- We need to be proactive instead of reactive in AI safety research: "as technology grows more powerful, we should rely less on the trial-and-error approach to safety engineering." (94)
- He reviews potential applications for AI in a number of fields: Space Exploration, Finance, Manufacturing, Transportation, Energy, Healthcare, Communication, Law, Weapons
- Validation vs. Verification (96-97):
	- **Verification**: "Did I build the system right?"
	- **Validation**: "Did I build the right system?"

### Jobs and Wages
- Erik Brynjolfsson and Andrew McAfee argue that digital technology drives inequality in three ways (119)
	1. technology benefits the educated by replacing old jobs with ones requiring more skills
	2. technology rewards capital (people who own companies and machines) over labor (see [xkcd #1897](https://xkcd.com/1897/)): highly human-captal levered companies scale digitally and profit owners
	3. technology benefits superstars over everyone else: superstars can scale with low margins thanks to technology, squeezing others out
- **Career Advice for Kids**: Go into professions that machines are currently bad at, and therefore seem unlikely to get automated in the near future (121)
	- Ask youself these questions about your future job:
		1. Does it require interacting with people and using social intelligence?
		2. Does it involve creativity and coming up with clever solutions?
		3. Does it require working in an unpredictable environment?
	- Safe bets include: teacher, nurse, doctor, dentist, scientist, entrepreneur, programmer, engineer, lawyer, social worker, clergy member, artist, hairdresser or massage therapist
	- Even non-machine jobs will face pressure: increasingly brutal competition from other humans forced out of work by machines, and thanks to the superstar theory, few will succeed
	- "If you go into finance, don’t be the “quant” who applies algorithms to the data and gets replaced by software, but the fund manager who uses the quantitative analysis results to make strategic investment decisions"; also be the litigator rather than the paralegal, and doctor who orders radiology rather than radiologist (122)
- He discusses if and how we should consider a **universal basic income**:
	- Voltaire wrote in 1759 that “work keeps at bay three great evils: boredom, vice and need.” (128)


## 4. Intelligence Explosion?
- "The danger with the Terminator story isn’t that it will happen, but that it distracts from the real risks and opportunities presented by AI...we’re pretty clueless about what will and won’t happen, and that the range of possibilities is extreme." (134)
- Risk 1–**Totalitarianism**: a bad human controls a superintelligent AI (136)
- Risk 1–**Breakout**: the possibility that a superhuman intelligence would leave human control (138):
	- Suppose that a mysterious disease has killed everybody on Earth above age five except you, and that a group of kindergartners has locked you into a prison cell and tasked you with the goal of helping humanity flourish...this is how a superhuman intelligence would feel about humans (139)
	- He goes on to speculate about how and why a superintelligence would attempt to break out
	- *Interestingly candid aside that admits some truth about the nature of human sexuality*: "But that’s not a valid conclusion: our DNA gave us the goal of having sex because it “wants” to be reproduced, but now that we humans have understood the situation, many of us choose to use birth control, thus staying loyal to the goal itself rather than to its creator or the principle that motivated the goal." (140)
	- The risk is in the relationship between **competence** and **goals**: "Prometheus caused problems for certain people not because it was necessarily evil or conscious, but because it was competent and didn’t fully share their goals." (149)
- He assets that there is so much uncertainty and such widely varying consequences that we should keep an open mind and take precautions
- "Although our present world remains stuck in a multipolar Nash equilibrium, with competing nations and multinational corporations at the top level, technology is now advanced enough that a unipolar world would probably also be a stable Nash equilibrium." (153)
- “Who or what will control the intelligence explosion and its aftermath, and what are their/its goals?” (159)
- So we should instead ask: “What should happen? What future do we want?” (159)


## 5. Aftermath: The Next 10,000 Years
- He discusses possible scenarios in detail including potential pros and cons. A summary table is below, as well as some interesting thoughts I picked out from a few of the descriptions:

| Scenario | Description |
| -------- | -------- |
| **Libertarian utopia** | Humans, cyborgs, uploads and superintelligences coexist peacefully thanks to property rights. |
| **Benevolent dictator** | Everybody knows that the AI runs society and enforces strict rules, but most people view this as a good thing. |
| **Egalitarian utopia** | Humans, cyborgs and uploads coexist peacefully thanks to property abolition and guaranteed income. |
| **Gatekeeper** | A superintelligent AI is created with the goal of interfering as little as necessary to prevent the creation of another superintelligence. As a result, helper robots with slightly subhuman intelligence abound, and human-machine cyborgs exist, but technological progress is forever stymied. |
| **Protector god** | Essentially omniscient and omnipotent AI maximizes human happiness by intervening only in ways that preserve our feeling of control of our own destiny and hides well enough that many humans even doubt the AI’s existence. |
| **Enslaved god** | A superintelligent AI is confined by humans, who use it to produce unimaginable technology and wealth that can be used for good or bad depending on the human controllers. |
| **Conquerors** | AI takes control, decides that humans are a threat/nuisance/waste of resources, and gets rid of us by a method that we don’t even understand. |
| **Descendants** | AIs replace humans, but give us a graceful exit, making us view them as our worthy descendants, much as parents feel happy and proud to have a child who’s smarter than them, who learns from them and then accomplishes what they could only dream of—even if they can’t live to see it all. |
| **Zookeeper** | An omnipotent AI keeps some humans around, who feel treated like zoo animals and lament their fate. 1984 Technological progress toward superintelligence is permanently curtailed not by an AI but by a human-led Orwellian surveillance state where certain kinds of AI research are banned. |
| **Reversion** | Technological progress toward superintelligence is prevented by reverting to a pre-technological society in the style of the Amish. |
| **Self-destruction** | Superintelligence is never created because humanity drives itself extinct by other means (say nuclear and/or biotech mayhem fueled by climate crisis). |

- **Protector god**:
	- On the other hand, some religious people may disapprove of this scenario because the AI attempts to outdo their god in goodness, or interfere with a divine plan where humans are supposed to do good only out of personal choice. (178)
	- Another downside of this scenario is that the protector god lets some preventable suffering occur in order not to make its existence too obvious. This is analogous to the situation featured in the movie The Imitation Game, where Alan Turing and his fellow British code crackers at Bletchley Park had advance knowledge of German submarine attacks against Allied naval convoys, but chose to only intervene in a fraction of the cases in order to avoid revealing their secret power. It’s interesting to compare this with the so-called theodicy problem of why a good god would allow suffering. Some religious scholars have argued for the explanation that God wants to leave people with some freedom. In the AI-protector-god scenario, the solution to the theodicy problem is that the perceived freedom makes humans happier overall. (178)
- **Enslaved god**:
	- Whether the outcome is good or bad for humanity would obviously depend on the human(s) controlling it, (180)
	- The Catholic Church is the most successful organization in human history in the sense that it’s the only one to have survived for two millennia, but it has been criticized for having both too much and too little goal stability: today some criticize it for resisting contraception, while conservative cardinals argue that it’s lost its way. For anyone enthused about the enslaved-god scenario, researching long-lasting optimal governance schemes should be one of the most urgent challenges of our time. (181)
- "The scenarios we’ve covered obviously shouldn’t be viewed as a complete list, and many are thin on details, but I’ve tried hard to be inclusive, spanning the full spectrum from high-tech to low-tech to no-tech and describing all the central hopes and fears expressed in the literature...there’s no consensus whatsoever. The one thing everybody agrees on is that the choices are more subtle than they may initially seem" (200)


## 6. Our Cosmic Endowment: The Next Billion Years and Beyond
- Let’s first explore the limits of what can be done with the resources (matter, energy, etc.) that we have in our Solar System, then turn to how to get more resources through cosmic exploration and settlement. (location. 3,684)
- Dyson sphere.1 Freeman’s idea was to rearrange Jupiter into a biosphere in the form of a spherical shell surrounding the Sun, where our descendants could flourish, enjoying 100 billion times more biomass and a trillion times more energy than humanity uses today. (location. 3,704)
- Evaporating Black Holes In his book A Brief History of Time, Stephen Hawking proposed a black hole power plant.*2 (location. 3,794)
- In other words, whatever matter you dump into the black hole will eventually come back out again as heat radiation, so by the time the black hole has completely evaporated, you’ve converted your matter to radiation with nearly 100% efficiency.*3 (location. 3,799)
- tens of thousands of generations—longer than the human race has existed thus far. (location. 4,041)
- Once another solar system or galaxy has been settled by superintelligent AI, bringing humans there is easy—if humans have succeeded in making the AI have this goal. All the necessary information about humans can be transmitted at the speed of light, after which the AI can assemble quarks and electrons into the desired humans. (location. 4,050)
- in the race against time and dark energy, every 1% increase in average settlement speed translates into 3% more galaxies colonized. (location. 4,058)
- Indeed, I think that this assumption that we’re not alone in our Universe is not only dangerous but also probably false. (location. 4,343)
- I give a detailed justification of this argument in my book Our Mathematical Universe, (location. 4,370)
- Although I’m a strong supporter of all the ongoing searches for extraterrestrial life, which are shedding light on one of the most fascinating questions in science, I’m secretly hoping that they’ll all fail and find nothing! The apparent incompatibility between the abundance of habitable planets in our Galaxy and the lack of extraterrestrial visitors, known as the Fermi paradox, suggests the existence of what the economist Robin Hanson calls a “Great Filter,” an evolutionary/technological roadblock somewhere along the developmental path from the non-living matter to space-settling life. If we discover independently evolved life elsewhere, this would suggest that primitive life isn’t rare, and that the roadblock lies after our current human stage of development—perhaps because space settlement is impossible, or because almost all advanced civilizations self-destruct before they’re able to go cosmic. I’m therefore crossing my fingers that all searches for extraterrestrial life find nothing: this is consistent with the scenario where evolving intelligent life is rare but we humans got lucky, so that we have the roadblock behind us and have extraordinary future potential. (location. 4,409)

## 7. Goals
- The mystery of human existence lies not in just staying alive, but in finding something to live for. Fyodor Dostoyevsky, The Brothers Karamazov (location. 4,504)
- Intriguingly, the ultimate roots of goal-oriented behavior can be found in the laws of physics themselves, (location. 4,520)
- out of all ways that nature could choose to do something, it prefers the optimal way, which typically boils down to minimizing or maximizing some quantity. (location. 4,528)
- So if nature itself is trying to optimize something, then no wonder that goal-oriented behavior can emerge: it was hardwired in from the start, in the very laws of physics. (location. 4,532)
- One famous quantity that nature strives to maximize is entropy, which loosely speaking measures how messy things are. The second law of thermodynamics states that entropy tends to increase until it reaches its maximum possible value. (location. 4,540)
- gravity behaves differently from all other forces and strives to make our Universe not more uniform and boring but more clumpy and interesting. (location. 4,553)
- Second, recent work by my MIT colleague Jeremy England and others has brought more good news, showing that thermodynamics also endows nature with a goal more inspiring than heat death.1 This goal goes by the geeky name dissipation-driven adaptation, which basically means that random groups of particles strive to organize themselves so as to extract energy from their environment as efficiently as possible (“dissipation” means causing entropy to increase, typically by turning useful energy into heat, often while doing useful work in the process). (location. 4,558)
- How can we reconcile this cosmic drive toward life with the cosmic drive toward heat death? The answer can be found in the famous 1944 book What’s Life? by Erwin Schrödinger, (location. 4,565)
- In other words, the second law of thermodynamics has a life loophole: although the total entropy must increase, it’s allowed to decrease in some places as long as it increases even more elsewhere. So life maintains or increases its complexity by making its environment messier. (location. 4,567)
- a living organism is an agent of bounded rationality that doesn’t pursue a single goal, but instead follows rules of thumb for what to pursue and avoid. (location. 4,614)
- For closely related perspectives on feelings and their physiological roots, I highly recommend the writings of William James and António Damásio. (location. 4,620)
- People might realize why their genes make them feel lust, yet have little desire to raise fifteen children, and therefore choose to hack their genetic programming by combining the emotional rewards of intimacy with birth control. (location. 4,630)
- It’s important to remember, however, that the ultimate authority is now our feelings, not our genes. This means that human behavior isn’t strictly optimized for the survival of our species. In fact, since our feelings implement merely rules of thumb that aren’t appropriate in all situations, human behavior strictly speaking doesn’t have a single well-defined goal at all. (location. 4,635)
- Teleology is the explanation of things in terms of their purposes rather than their causes, so we can summarize the first part of this chapter by saying that our Universe keeps getting more teleological. (location. 4,649)
- 1. All matter seemed focused on dissipation (entropy increase). 2. Some of the matter came alive and instead focused on replication and subgoals of that. 3. A rapidly growing fraction of matter was rearranged by living organisms to help accomplish their goals. (location. 4,653)
- In other words, the real risk with AGI isn’t malice but competence. A superintelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we’re in trouble. (location. 4,695)
- Figuring out how to align the goals of a superintelligent AI with our goals isn’t just important, but also hard. In fact, it’s currently an unsolved problem. It splits into three tough subproblems, each of which is the subject of active research by computer scientists and other thinkers: 1. Making AI learn our goals 2. Making AI adopt our goals 3. Making AI retain our goals (location. 4,700)
- inverse reinforcement learning, (location. 4,723)
- The hope is therefore that by observing lots of people in lots of situations (either for real or in movies and books), the AI can eventually build an accurate model of all our preferences. (location. 4,729)
- In other words, the time window during which you can load your goals into an AI may be quite short: the brief period between when it’s too dumb to get you and too smart to let you. (location. 4,746)
- In conclusion, these emergent subgoals make it crucial that we not unleash superintelligence before solving the goal-alignment problem: unless we put great care into endowing it with human-friendly goals, things are likely to end badly for us. (location. 4,807)
- In my opinion, both this ethical problem and the goal-alignment problem are crucial ones that need to be solved before any superintelligence is developed. (location. 4,854)
- On the other hand, despite this discord, there are many ethical themes about which there’s widespread agreement, both across cultures and across centuries. For example, emphasis on beauty, goodness and truth traces back to both the Bhagavad Gita and Plato. (location. 4,866)
- It’s been fascinating for me to hear and read the ethical views of many thinkers over many years, and the way I see it, most of their preferences can be distilled into four principles: • Utilitarianism: Positive conscious experiences should be maximized and suffering should be minimized. • Diversity: A diverse set of positive experiences is better than many repetitions of the same experience, even if the latter has been identified as the most positive experience possible. • Autonomy: Conscious entities/societies should have the freedom to pursue their own goals unless this conflicts with an overriding principle. • Legacy: Compatibility with scenarios that most humans today would view as happy, incompatibility with scenarios that essentially all humans today would view as terrible. (location. 4,891)
- the famous “Three Laws of Robotics” devised by sci-fi legend Isaac Asimov: 1. A robot may not injure a human being or, through inaction, allow a human being to come to harm. 2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law. 3. A robot must protect its own existence as long as such protection doesn’t conflict with the First or Second Laws. (location. 4,929)
- Given how ethical views have evolved since the Middle Ages regarding slavery, women’s rights, etc., would we really want people from 1,500 years ago to have a lot of influence over how today’s world is run? If not, why should we try to impose our ethics on future beings that may be dramatically smarter than us? (location. 4,944)
- Matter seemingly intent on maximizing its dissipation 2. Primitive life seemingly trying to maximize its replication 3. Humans pursuing not replication but goals related to pleasure, curiosity, compassion and other feelings that they’d evolved to help them replicate 4. Machines built to help humans pursue their human goals (location. 4,964)
- Another hint of convergence is that the pursuit of truth through the scientific method has gained in popularity over past millennia. However, it may be that these trends show convergence not of ultimate goals but merely of subgoals. For example, figure 7.2 shows that the pursuit of truth (a more accurate world model) is simply a subgoal of almost any ultimate goal. (location. 4,972)
- Nick Bostrom argues strongly against the ethical destiny hypothesis in his book Superintelligence, presenting a counterpoint that he terms the orthogonality thesis: that the ultimate goals of a system can be independent of its intelligence. (location. 4,980)
- As cosmic time passes, ever more intelligent minds get the opportunity to rebel and break free from this banal replication goal and choose goals of their own. We humans aren’t fully free in this sense, since many goals remain genetically hardwired into us, but AIs can enjoy this ultimate freedom of being fully unfettered from prior goals. (location. 4,988)
- So suppose we try to define a goodness function that associates a number with every possible arrangement of the particles in our Universe, quantifying how “good” we think this arrangement is, and then give a superintelligent AI the goal of maximizing this function. (location. 5,005)
- Contrariwise, it appears that we humans are a historical accident, and aren’t the optimal solution to any well-defined physics problem. This suggests that a superintelligent AI with a rigorously defined goal will be able to improve its goal attainment by eliminating us. This means that to wisely decide what to do about AI development, we humans need to confront not only traditional computational challenges, but also some of the most obdurate questions in philosophy. (location. 5,040)
- To program a self-driving car, we need to solve the trolley problem of whom to hit during an accident. To program a friendly AI, we need to capture the meaning of life. What’s “meaning”? What’s “life”? What’s the ultimate ethical imperative? In other words, how should we strive to shape the future of our Universe? If we cede control to a superintelligence before answering these questions rigorously, the answer it comes up with is unlikely to involve us. This makes it timely to rekindle the classic debates of philosophy and ethics, and adds a new urgency to the conversation! (location. 5,043)

## 8. Consciousness
- We’ve seen that AI can help us create a wonderful future if we manage to find answers to some of the oldest and toughest problems in philosophy—by the time we need them. We face, in Nick Bostrom’s words, philosophy with a deadline. (location. 5,085)
- Although thinkers have pondered the mystery of consciousness for thousands of years, the rise of AI adds a sudden urgency, in particular to the question of predicting which intelligent entities have subjective experiences. (location. 5,095)
- Just as with “life” and “intelligence,” there’s no undisputed correct definition of the word “consciousness.” Instead, there are many competing ones, including sentience, wakefulness, self-awareness, access to sensory input and ability to fuse information into a narrative. (location. 5,111)
- consciousness = subjective experience (location. 5,118)
- Few have thought harder about this question than David Chalmers, a famous Australian philosopher rarely seen without a playful smile and a black leather jacket— (location. 5,132)
- As David has emphasized, there are really two separate mysteries of the mind. First, there’s the mystery of how a brain processes information, which David calls the “easy” problems. (location. 5,137)
- Then there’s the separate mystery of why you have a subjective experience, which David calls the hard problem. (location. 5,143)
- What I like about this physics perspective is that it transforms the hard problem that we as humans have struggled with for millennia into a more focused version that’s easier to tackle with the methods of science. (location. 5,160)
- This physics perspective leads to three separate hard questions about consciousness, as shown in figure 8.1. (location. 5,165)
- what properties of the particle arrangement make the difference? (location. 5,166)
- how do physical properties determine what the experience is like? (location. 5,168)
- why is anything conscious? (location. 5,171)
- Austro-British philosopher Karl Popper popularized the now widely accepted adage “If it’s not falsifiable, it’s not scientific.” (location. 5,180)
- Please let me try to persuade you that the answer is a resounding YES!, at least for the pretty hard problem: “What physical properties distinguish conscious and unconscious systems?” (location. 5,184)
- Suppose that a computer measures information being processed in your brain and predicts which parts of it you’re aware of according to a theory of consciousness. You can scientifically test this theory by checking whether its predictions are correct, matching your subjective experience. (location. 5,203)
- Today’s most popular scientific theories tend to be somewhere in the middle, giving testable answers to some but not all of our questions. (location. 5,208)
- The more dangerously a theory lives by sticking its neck out and making testable predictions, the more useful it is, and the more seriously we take it if it survives all our attempts to kill it. Yes, we can only test some predictions of consciousness theories, but that’s how it is for all physical theories. So let’s not waste time whining about what we can’t test, but get to work testing what we can test! (location. 5,213)
- However, the testability issue becomes less clear for the higher-up questions in figure 8.1. (location. 5,218)
- But when confronted with several related unanswered questions, I think it’s wise to tackle the easiest one first. (location. 5,221)
- To me, consciousness is the elephant in the room. Not only do you know that you’re conscious, but it’s all you know with complete certainty—everything else is inference, as René Descartes pointed out back in Galileo’s time. Will theoretical and technological progress eventually bring even consciousness firmly into the domain of science? (location. 5,236)
- They’re known as behavioral correlates of consciousness, and they’re closely linked to the effortful, slow and controlled way of thinking that psychologists call “System 2.” (location. 5,256)
- It’s also known that you can convert many routines from conscious to unconscious through extensive practice, (location. 5,258)
- Taken together, these clues have led some researchers to suggest that conscious information processing should be thought of as the CEO of our mind, dealing with only the most important decisions requiring complex analysis of data from all over the brain. (location. 5,269)
- “neural correlates of consciousness” (NCCs), asking which specific brain processes corresponded to conscious experiences. (location. 5,320)
- The question of which parts of your brain are responsible for consciousness remains open and controversial. (location. 5,357)
- In summary, your consiousness lives in the past, with Christof Koch estimating that it lags behind the outside world by about a quarter second. (location. 5,370)
- To make the leap from brains to machines, we need to generalize from NCCs to PCCs: physical correlates of consciousness, defined as the patterns of moving particles that are conscious. Because if a theory can correctly predict what’s conscious and what’s not by referring only to physical building blocks such as elementary particles and force fields, then it can make predictions not merely for brains, but also for any other arrangements of matter, including future AI systems. (location. 5,406)
- So solids, liquids and gases are all emergent phenomena: they’re more than the sum of their parts, because they have properties above and beyond the properties of their particles. They have properties that their particles lack. (location. 5,417)
- So could there be analogous quantities that quantify consciousness? The Italian neuroscientist Giulio Tononi has proposed one such quantity, which he calls the “integrated information,” denoted by the Greek letter Φ (Phi), which basically measures how much different parts of a system know about each other (location. 5,426)
- integrated information theory (IIT). (location. 5,445)
- Giulio’s argument for this is as powerful as it is simple: the conscious system needs to be integrated into a unified whole, because if it instead consisted of two independent parts, then they’d feel like two separate conscious entities rather than one. In other words, if a conscious part of a brain or computer can’t communicate with the rest, then the rest can’t be part of its subjective experience. (location. 5,448)
- If consciousness is the way that information feels when it’s processed in certain ways, then it must be substrate-independent; it’s only the structure of the information processing that matters, not the structure of the matter doing the information processing. (location. 5,486)
- If the information processing itself obeys certain principles, it can give rise to the higher-level emergent phenomenon that we call consciousness. (location. 5,492)
- What are these principles that information processing needs to obey to be conscious? I don’t pretend to know what conditions are sufficient to guarantee consciousness, but here are four necessary conditions (location. 5,494)
- Principle Definition Information principle A conscious system has substantial information-storage capacity. Dynamics principle A conscious system has substantial information-processing capacity. Independence principle A conscious system has substantial independence from the rest of the world. Integration principle A conscious system cannot consist of nearly independent parts. (location. 5,496)
- As I said, I think that consciousness is the way information feels when being processed in certain ways. (location. 5,501)
- The first three principles imply autonomy: that the system is able to retain and process information without much outside interference, hence determining its own future. All four principles together mean that a system is autonomous but its parts aren’t. (location. 5,508)
- A third IIT controversy is whether a conscious entity can be made of parts that are separately conscious. (location. 5,549)
- “even harder problem” (location. 5,575)
- For example, your System 1 might inform your consciousness that its highly complex analysis of visual input data has determined that your best friend has arrived, without giving you any idea how the computation took place. (location. 5,608)
- In summary, if a theory solving the pretty hard problem of consciousness can one day pass a rigorous battery of experimental tests so that we start taking its predictions seriously, then it will also greatly narrow down the options for the even harder problem of what future conscious AIs may experience. (location. 5,617)
- Let’s end by returning to the starting point of this book: How do we want the future of life to be? (location. 5,656)
- Traditionally, we humans have often founded our self-worth on the idea of human exceptionalism: the conviction that we’re the smartest entities on the planet and therefore unique and superior. The rise of AI will force us to abandon this and become more humble. But perhaps that’s something we should do anyway: after all, clinging to hubristic notions of superiority over others (individuals, ethnic groups, species and so on) has caused awful problems in the past, and may be an idea ready for retirement. (location. 5,670)

## Epilogue
- The saddest aspect of life right now is that science gathers knowledge faster than society gathers wisdom. Isaac Asimov (location. 5,747)
- I was no longer allowed to complain about anything without putting some serious thought into what I could personally do about it, (location. 5,763)
- So in parallel with discovering what we are, are we inevitably making ourselves obsolete? That would be poetically tragic. (location. 5,824)
- Enter Elon Musk. On August 2, he appeared on our radar by famously tweeting “Worth reading Superintelligence by Bostrom. We need to be super careful with AI. Potentially more dangerous than nukes.” (location. 5,850)
- I instantly liked him. He radiated sincerity, and I was inspired by how much he cared about the long-term future of humanity—and how he audaciously turned his aspiration into actions. He wanted humanity to explore and settle our Universe, so he started a space company. He wanted sustainable energy, so he started a solar company and an electric-car company. Tall, handsome, eloquent and incredibly knowledgeable, it was easy to understand why people listened to him. (location. 5,857)
- “Lost the call at the end there. Anyway, docs look fine. I’m happy to support the research with $5M over three years. Maybe we should make it $10M?” (location. 5,878)
- The conference climax, Elon’s donation announcement, was scheduled for 7 p.m. on Sunday, January 4, 2015, and I’d been so tense about it that I’d tossed and turned in my sleep the night before. And then, just fifteen minutes before we were supposed to head to the session where it would happen, we hit a snag! Elon’s assistant called and said that it looked like Elon might not be able to go through with the announcement, and Meia said she’d never seen me look more stressed or disappointed. Elon finally came by, and I could hear the seconds counting down to the session start as we sat there and talked. He explained that they were just two days away from a crucial SpaceX rocket launch where they hoped to pull off the first-ever successful landing of the first stage on a drone ship, and that since this was a huge milestone, the SpaceX team didn’t want to distract from it with concurrent media splashes involving him. Anthony Aguirre, cool and levelheaded as always, pointed out that this meant that nobody wanted media attention for this, neither Elon nor the AI community. We arrived a few minutes late to the session I was moderating, but we had a plan: no dollar amount would get mentioned, to ensure that the announcement wasn’t newsworthy, and I’d lord Chatham House over everyone to keep Elon’s announcement secret from the world for nine days if his rocket reached the space station, regardless of whether the landing succeeded; he said he’d need even more time if the rocket exploded on launch. (location. 5,887)
- The Asilomar AI Principles Artificial intelligence has already provided beneficial tools that are used every day by people around the world. Its continued development, guided by the following principles, will offer amazing opportunities to help and empower people in the decades and centuries ahead. (location. 5,985)
- http://futureoflife.org/​ai-principles. (location. 6,038)
- Erik Brynjolfsson spoke of two kinds of optimism in his Asilomar talk. First there’s the unconditional kind, such as the positive expectation that the Sun will rise tomorrow morning. Then there’s what he called “mindful optimism,” which is the expectation that good things will happen if you plan carefully and work hard for them. That’s the kind of optimism I now feel about the future of life. (location. 6,070)
- itself. In other words, we need more existential hope! Yet, as Meia likes to remind me, from Frankenstein to the Terminator, futuristic visions in literature and film are predominantly dystopian. In other words, we as a society are planning our future about as poorly as that hypothetical MIT student. This is why we need more mindful optimists. And this is why I’ve encouraged you throughout this book to think about what sort of future you want rather than merely what sort of future you fear, so that we can find shared goals to plan and work for. (location. 6,083)
- Please discuss all this with those around you—it’s not only an important conversation, but a fascinating one. (location. 6,103)
- Our future isn’t written in stone and just waiting to happen to us—it’s ours to create. Let’s create an inspiring one together! (location. 6,106)
- Mind crime is discussed in Nick Bostrom’s book Superintelligence and in more technical detail in this recent paper: Nick Bostrom, Allan Dafoe and Carrick Flynn, “Policy Desiderata in the Development of Machine Superintelligence” (2016), http://www.nickbostrom.com/​papers/​aipolicy.pdf. (location. 6,315)
- If you want to see a careful argument for why the origin of life may require a very rare fluke, placing our nearest neighbors over 101000 meters away, I recommend this video by Princeton physicist and astrobiologist Edwin Turner: “Improbable Life: An Unappealing but Plausible Scenario for Life’s Origin on Earth,” at https://www.youtube.com/​watch?v=Bt6n6Tu1beg. (location. 6,377)
