
# [*Business Data Science: Combining Machine Learning and Economics to Optimize, Automate, and Accelerate Business*](https://www.amazon.com/Business-Data-Science-Combining-Accelerate/dp/1260452778/ref=sr_1_3?dchild=1&keywords=Business+Data+Science&qid=1618600384&sr=8-3) by Matt Taddy

(New York: McGraw Hill, 2019), 331

### Resources
- *The author's data and R scripts for the books is on [GitHub](https://github.com/TaddyLab/bds) (see also code for [MBA course](https://github.com/TaddyLab/MBA)).*
- *My code to accompany these notes is on [GitHub](https://github.com/mkudija/taddy-business-data-science).*
- *Amazon [author interview](https://www.amazon.science/business-data-science-is-a-lot-more-than-just-making-predictions-matt-taddy) discussing concepts from the book.ou*

## Preface

## Introduction
- The Goal: produce an *interpretable* model that translates raw data into information relevant to decision-making; project information into a low-dimensional space that contains key insights for decision making (3-4)
  - Motivating example: visualized CAPM outputs give a richer view for decisions making than just a messy plot of returns (Figure I.3, [reproduced here](https://github.com/mkudija/taddy-business-data-science/blob/main/0-introduction/3-stock-returns/0-introduction_stock-returns.ipynb))
- "Modern methods" are distinguished by *big data* and *machine learning*
  - Data Science â‰ˆ statistics + BD + ML
  - Business Data Science â‰ˆ statistics + BD + ML + economics + econometrics + business context
- **Big Data**: data can be "big" in terms of both *volume* and *complexity*
  - Big (*volume*) Data is where the scale swamps RAM and requires piping by data engineers
  - Big (*complexity*) Data is big *dimension* data where the assumptions of classical statistics break down ("big *p*" problems)
- **Machine Learning**: automatically build *predictions* from complex data
  - Focus is to maximize predictive performance on out-of-sample data
  - Limited *structural* interpretability: black box for making predictions when the future follows the same patterns as the past (with the implicit warning about the danger of changing patterns)
  - *Structural analysis* refers to building analytically from theory, as compared with the pragmatic, black-box *prediction* of machine learning
  - Good data science then is having an overall understanding of the domain to know the appropriate *prediction* tasks to throw at ML, and the *structural* problems to address with classical economics and statistics
  - "A policy-maker who can deploy ML to solve the many prediction tasks that they face will be able to automate and accelerate their decision process." (7)
  - ML prediction tools should be part of a larger system with goals beyond pure prediction
- **Computation**
  - This book uses R, but the key point is that "anyone working with data will need to continue learning and updating their computational (and methodological) skills"; best way to learn is by doing (11)


## Chapter 1: Uncertainty
*Summary: *

- **Parametric** vs **Nonparametric**
  - **Parametric**: "theoretical"
    - conditional on assumed true model
  - **Nonparametric**: "data driven"
    - allows for model misspecification
  

| Parametric             | Nonparametric          |
| ---------------------- | ---------------------- |
| Quantifies uncertainty | Quantifies uncertainty |
| "Theoretical"          | "Data-driven"          |
| CLT/Gaussian/Bayesian  | Bootstrap              |
| Specify distribution   | More flexible          |


- **Frequentist** vs **Bayesian**
  - **Frequentist**: classical uncertainty
    - *"If I were able to see a new sample of data generated by the same processes and scenarios as my current data, how would my estimates change?"*
  - **Bayesian**: mathematical framework of beliefs (more below)
    - *"If you believe A and then you observe B, you should update your beliefs to C."*
- **Central Limit Theorem** (CLT): the average of independent random variables becomes normally distributed if your sample size is large enough
- **The Bootstrap**: uses resampling (*with replacement*) from your current sample to mimic the sampling distribution and introduce variability
  - use the Bootstrap because theoretical Gaussian distributions derived from the CLT are not valid for many complex settings (i.e. number of model parameters large relative to number of observations)
  - the Bootstrap will work in many settings where theory is either unavailable or incorrect (if the Bootstrap fails, there probably isn't a good theoretical replacement)
  - an alternative is the **Parametric Bootstrap**: generate new data by drawing from a fitted model (but results sensitive to how well the model represents reality)
- ***p*-values**: represents how rare or strange your sample would be if the null hypothesis is true
  - the proportion of times that you would wrongly reject your safe null if the test statistic you've observed is enough to lead you to adopt the alternative
  - measures the probability mass in the tails past your observed test statistic
- **Benjamini-Hochberg (BH) FDR Control**: controls your false discovery rate (FDR) by defining a cutoff on the ranked list of *p*-values from your model
  - $FDR=\mathop{\mathbb{E}} \left[ \frac{FalsePositives}{TestsCalledSignificant} \right]$
  - Select your target $FDR$ of $q$, and keep all *p*-values such that $p \leq q\frac{k}{N}$
  - Gives a decent (often conservative) guess at FDR

- **Bayesian Inference**: the mathematical framework of beliefs
  - characterizes probabilities over models and parameters by appealing to the idea of subjective beliefs rather than repeated trials
  - quantification of risks and expected rewards is inherently Bayesian
  - "If you believe *A* and then you observe *B*, you should update your beliefs to *C*."
  - provides a framework for combining *assumptions* with *evidence*
  - mechanically works with combination of *prior distributions* and *likelihood* (the probability of the data you have observed given a fixed set of mode parameters)
  - inherently *parametric* since you specify a model and then update view of uncertainty based on new data
- 


## Chapter 2: Regression
*Summary: *



## Chapter 3: Regularization
*Summary: *

- "Regularization is trading variance (noise) for bias" (91)


## Chapter 4: Classification
*Summary: *



## Chapter 5: Experiments
*Summary: *



## Chapter 6: Controls
*Summary: *



## Chapter 7: Factorization
*Summary: *



## Chapter 8: Text as Data
*Summary: *



## Chapter 9: Nonparametrics
*Summary: *



## Chapter 10: Artificial Intelligence
*Summary: *


---

**Errata**
- 37: "is then available via Bayes rule" --> "is then available via **Bayes'** rule"
- 78: "in some data-dependent matter" --> "in some data-dependent **manner**"